{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcUAAAEjCAYAAABD3BobAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAEAcP4f9m7+4Orh5AkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x85e8f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 무작위(랜덤) 초기화 후의 가중치\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.29984078216\n",
      "=== epoch:1, train acc:0.162, test acc:0.135 ===\n",
      "train loss:2.29792170683\n",
      "train loss:2.29284621127\n",
      "train loss:2.28669777883\n",
      "train loss:2.28357993657\n",
      "train loss:2.27461831232\n",
      "train loss:2.26365301064\n",
      "train loss:2.24828159816\n",
      "train loss:2.22171426749\n",
      "train loss:2.20688877958\n",
      "train loss:2.16408499105\n",
      "train loss:2.12289847974\n",
      "train loss:2.07722876202\n",
      "train loss:2.08568898227\n",
      "train loss:1.95958455885\n",
      "train loss:1.93200891241\n",
      "train loss:1.86425647506\n",
      "train loss:1.8589332532\n",
      "train loss:1.82208318969\n",
      "train loss:1.66180156848\n",
      "train loss:1.6480220611\n",
      "train loss:1.50758522279\n",
      "train loss:1.45808297142\n",
      "train loss:1.45304497889\n",
      "train loss:1.34909685383\n",
      "train loss:1.22166768407\n",
      "train loss:1.14716586396\n",
      "train loss:1.07459096105\n",
      "train loss:0.971591555196\n",
      "train loss:1.00599333302\n",
      "train loss:0.941547076567\n",
      "train loss:0.903192005064\n",
      "train loss:0.879088008739\n",
      "train loss:0.874078596477\n",
      "train loss:0.81911385769\n",
      "train loss:0.726826730306\n",
      "train loss:0.759556359281\n",
      "train loss:0.773775289213\n",
      "train loss:0.834255422303\n",
      "train loss:0.663052717158\n",
      "train loss:0.797924356064\n",
      "train loss:0.676980540704\n",
      "train loss:0.771682160049\n",
      "train loss:0.62658935084\n",
      "train loss:0.714464692062\n",
      "train loss:0.539421424739\n",
      "train loss:0.74428506027\n",
      "train loss:0.783087777534\n",
      "train loss:0.590254369443\n",
      "train loss:0.501064421795\n",
      "train loss:0.591563855565\n",
      "=== epoch:2, train acc:0.808, test acc:0.787 ===\n",
      "train loss:0.555575696753\n",
      "train loss:0.552434704259\n",
      "train loss:0.90613222509\n",
      "train loss:0.673430090244\n",
      "train loss:0.414486089095\n",
      "train loss:0.548894911644\n",
      "train loss:0.437477681577\n",
      "train loss:0.475922664214\n",
      "train loss:0.468555619541\n",
      "train loss:0.40640088495\n",
      "train loss:0.493384249976\n",
      "train loss:0.700927072235\n",
      "train loss:0.381518864377\n",
      "train loss:0.379175885202\n",
      "train loss:0.279256354667\n",
      "train loss:0.392080867047\n",
      "train loss:0.497258680824\n",
      "train loss:0.40573888931\n",
      "train loss:0.563223968716\n",
      "train loss:0.48210712861\n",
      "train loss:0.463945766546\n",
      "train loss:0.446820832313\n",
      "train loss:0.467405317292\n",
      "train loss:0.383157598566\n",
      "train loss:0.511206730035\n",
      "train loss:0.540296815557\n",
      "train loss:0.705067774754\n",
      "train loss:0.418892700647\n",
      "train loss:0.348126284993\n",
      "train loss:0.363460160038\n",
      "train loss:0.434997020543\n",
      "train loss:0.320924572272\n",
      "train loss:0.324667138272\n",
      "train loss:0.373823761696\n",
      "train loss:0.4899857173\n",
      "train loss:0.42168503745\n",
      "train loss:0.41951295147\n",
      "train loss:0.426046039455\n",
      "train loss:0.657928084365\n",
      "train loss:0.399137835787\n",
      "train loss:0.380717016668\n",
      "train loss:0.429442357023\n",
      "train loss:0.308565468912\n",
      "train loss:0.291616955787\n",
      "train loss:0.250184389736\n",
      "train loss:0.388005840752\n",
      "train loss:0.325145567754\n",
      "train loss:0.260242964305\n",
      "train loss:0.509089649801\n",
      "train loss:0.211561763256\n",
      "=== epoch:3, train acc:0.862, test acc:0.849 ===\n",
      "train loss:0.310812208053\n",
      "train loss:0.581547167789\n",
      "train loss:0.338550192636\n",
      "train loss:0.514626860236\n",
      "train loss:0.218212403775\n",
      "train loss:0.486912249502\n",
      "train loss:0.326651102742\n",
      "train loss:0.573725240177\n",
      "train loss:0.408185415307\n",
      "train loss:0.38317863249\n",
      "train loss:0.326214406841\n",
      "train loss:0.248577731094\n",
      "train loss:0.34283313178\n",
      "train loss:0.30734863567\n",
      "train loss:0.276864700631\n",
      "train loss:0.334942048767\n",
      "train loss:0.234952690541\n",
      "train loss:0.233446763568\n",
      "train loss:0.450983133185\n",
      "train loss:0.332139089877\n",
      "train loss:0.537206402386\n",
      "train loss:0.340905493394\n",
      "train loss:0.463844369967\n",
      "train loss:0.412566012892\n",
      "train loss:0.285267721678\n",
      "train loss:0.350805080363\n",
      "train loss:0.248414775293\n",
      "train loss:0.288142863603\n",
      "train loss:0.485074578796\n",
      "train loss:0.483077585117\n",
      "train loss:0.276655738393\n",
      "train loss:0.334672444713\n",
      "train loss:0.392371289483\n",
      "train loss:0.259163508654\n",
      "train loss:0.286089123277\n",
      "train loss:0.371473727193\n",
      "train loss:0.308427513282\n",
      "train loss:0.369965465206\n",
      "train loss:0.279487174791\n",
      "train loss:0.255734444958\n",
      "train loss:0.202312417253\n",
      "train loss:0.220474556836\n",
      "train loss:0.345315427562\n",
      "train loss:0.301167843252\n",
      "train loss:0.32644668721\n",
      "train loss:0.209234396716\n",
      "train loss:0.304687518688\n",
      "train loss:0.282517495288\n",
      "train loss:0.30892362773\n",
      "train loss:0.298984713798\n",
      "=== epoch:4, train acc:0.891, test acc:0.86 ===\n",
      "train loss:0.311029664222\n",
      "train loss:0.28767714245\n",
      "train loss:0.223974498171\n",
      "train loss:0.383879605497\n",
      "train loss:0.312367853386\n",
      "train loss:0.308546461875\n",
      "train loss:0.40484414531\n",
      "train loss:0.292711509594\n",
      "train loss:0.258028729884\n",
      "train loss:0.187905245326\n",
      "train loss:0.287285294041\n",
      "train loss:0.325308940416\n",
      "train loss:0.285725098187\n",
      "train loss:0.183318215037\n",
      "train loss:0.183458217247\n",
      "train loss:0.248582804443\n",
      "train loss:0.303740170931\n",
      "train loss:0.257367136528\n",
      "train loss:0.252676146187\n",
      "train loss:0.187919928739\n",
      "train loss:0.354562983131\n",
      "train loss:0.308486934403\n",
      "train loss:0.231175072268\n",
      "train loss:0.416948316716\n",
      "train loss:0.187450852333\n",
      "train loss:0.323713146672\n",
      "train loss:0.3990194678\n",
      "train loss:0.282012491508\n",
      "train loss:0.167062113054\n",
      "train loss:0.158414083031\n",
      "train loss:0.197989719916\n",
      "train loss:0.213837269183\n",
      "train loss:0.195278802728\n",
      "train loss:0.188912442096\n",
      "train loss:0.426057723788\n",
      "train loss:0.426202199597\n",
      "train loss:0.204092804678\n",
      "train loss:0.471453606133\n",
      "train loss:0.147341430383\n",
      "train loss:0.311608548591\n",
      "train loss:0.279428052881\n",
      "train loss:0.273986362516\n",
      "train loss:0.254615544149\n",
      "train loss:0.236497597986\n",
      "train loss:0.239888480194\n",
      "train loss:0.382621327154\n",
      "train loss:0.222474199235\n",
      "train loss:0.161549304861\n",
      "train loss:0.226166325706\n",
      "train loss:0.158743120268\n",
      "=== epoch:5, train acc:0.916, test acc:0.898 ===\n",
      "train loss:0.351781354067\n",
      "train loss:0.276184039523\n",
      "train loss:0.252839292204\n",
      "train loss:0.288985415434\n",
      "train loss:0.368082705009\n",
      "train loss:0.248669171348\n",
      "train loss:0.200408857601\n",
      "train loss:0.174905202874\n",
      "train loss:0.28420417812\n",
      "train loss:0.246018663428\n",
      "train loss:0.260117268065\n",
      "train loss:0.166037295886\n",
      "train loss:0.247494678947\n",
      "train loss:0.215463933427\n",
      "train loss:0.240220921793\n",
      "train loss:0.161663378531\n",
      "train loss:0.302382009783\n",
      "train loss:0.233151857092\n",
      "train loss:0.426276693567\n",
      "train loss:0.245275742578\n",
      "train loss:0.346826832774\n",
      "train loss:0.133344381521\n",
      "train loss:0.244153573217\n",
      "train loss:0.127035934452\n",
      "train loss:0.248085166691\n",
      "train loss:0.137896063985\n",
      "train loss:0.125385775515\n",
      "train loss:0.341476310901\n",
      "train loss:0.224075238693\n",
      "train loss:0.151033834094\n",
      "train loss:0.308784075916\n",
      "train loss:0.388596710793\n",
      "train loss:0.257227263265\n",
      "train loss:0.226491228914\n",
      "train loss:0.296857424658\n",
      "train loss:0.200653234372\n",
      "train loss:0.18341185768\n",
      "train loss:0.246762901068\n",
      "train loss:0.259175618859\n",
      "train loss:0.281593077173\n",
      "train loss:0.162868995428\n",
      "train loss:0.187764387607\n",
      "train loss:0.288354727092\n",
      "train loss:0.264067410364\n",
      "train loss:0.120108571324\n",
      "train loss:0.195835321287\n",
      "train loss:0.265688933434\n",
      "train loss:0.163100728693\n",
      "train loss:0.198115622932\n",
      "train loss:0.16281305026\n",
      "=== epoch:6, train acc:0.917, test acc:0.905 ===\n",
      "train loss:0.300638897387\n",
      "train loss:0.221208387598\n",
      "train loss:0.225541708891\n",
      "train loss:0.407792171794\n",
      "train loss:0.21353548271\n",
      "train loss:0.171080293181\n",
      "train loss:0.364587807294\n",
      "train loss:0.231849196087\n",
      "train loss:0.12625396365\n",
      "train loss:0.193886757519\n",
      "train loss:0.172007305896\n",
      "train loss:0.278361783118\n",
      "train loss:0.190844981892\n",
      "train loss:0.283204524692\n",
      "train loss:0.121341232478\n",
      "train loss:0.218728690246\n",
      "train loss:0.154210059463\n",
      "train loss:0.332559965734\n",
      "train loss:0.0761283261405\n",
      "train loss:0.225117933779\n",
      "train loss:0.204231333459\n",
      "train loss:0.181074414964\n",
      "train loss:0.214691341905\n",
      "train loss:0.27101905695\n",
      "train loss:0.185530106401\n",
      "train loss:0.12981047563\n",
      "train loss:0.153197277457\n",
      "train loss:0.187370270275\n",
      "train loss:0.0996538765262\n",
      "train loss:0.202951246371\n",
      "train loss:0.269874723552\n",
      "train loss:0.144336534622\n",
      "train loss:0.123161707477\n",
      "train loss:0.215204829458\n",
      "train loss:0.199383784332\n",
      "train loss:0.250218160834\n",
      "train loss:0.179019573639\n",
      "train loss:0.147892367276\n",
      "train loss:0.252401603292\n",
      "train loss:0.156429296346\n",
      "train loss:0.0906135817273\n",
      "train loss:0.184081822071\n",
      "train loss:0.197838981415\n",
      "train loss:0.203507431501\n",
      "train loss:0.284000379922\n",
      "train loss:0.208686664074\n",
      "train loss:0.170603399139\n",
      "train loss:0.188761473111\n",
      "train loss:0.198117315817\n",
      "train loss:0.187051282463\n",
      "=== epoch:7, train acc:0.932, test acc:0.922 ===\n",
      "train loss:0.114771093679\n",
      "train loss:0.111328398383\n",
      "train loss:0.133293301797\n",
      "train loss:0.182201734852\n",
      "train loss:0.171924924567\n",
      "train loss:0.165690260572\n",
      "train loss:0.255640585689\n",
      "train loss:0.247934757488\n",
      "train loss:0.0694955169991\n",
      "train loss:0.0979104030196\n",
      "train loss:0.0965964100704\n",
      "train loss:0.148251523033\n",
      "train loss:0.192977567232\n",
      "train loss:0.19842511293\n",
      "train loss:0.104571020118\n",
      "train loss:0.163104062933\n",
      "train loss:0.139468070964\n",
      "train loss:0.42761016763\n",
      "train loss:0.269287600471\n",
      "train loss:0.291474896375\n",
      "train loss:0.107789718662\n",
      "train loss:0.0959345820821\n",
      "train loss:0.0826121004872\n",
      "train loss:0.235843040455\n",
      "train loss:0.324824716389\n",
      "train loss:0.277431966253\n",
      "train loss:0.102650739709\n",
      "train loss:0.176826653582\n",
      "train loss:0.111062960966\n",
      "train loss:0.304380049492\n",
      "train loss:0.18733991494\n",
      "train loss:0.111337718511\n",
      "train loss:0.187296096317\n",
      "train loss:0.366708723077\n",
      "train loss:0.109117582365\n",
      "train loss:0.109821994895\n",
      "train loss:0.244410223551\n",
      "train loss:0.14687513234\n",
      "train loss:0.192241905433\n",
      "train loss:0.0925297148035\n",
      "train loss:0.179356396939\n",
      "train loss:0.134984574861\n",
      "train loss:0.256428788079\n",
      "train loss:0.0799515509762\n",
      "train loss:0.167609819383\n",
      "train loss:0.159059188799\n",
      "train loss:0.133548958215\n",
      "train loss:0.132866022578\n",
      "train loss:0.199682657341\n",
      "train loss:0.125523716404\n",
      "=== epoch:8, train acc:0.948, test acc:0.92 ===\n",
      "train loss:0.143600662603\n",
      "train loss:0.128301005923\n",
      "train loss:0.159769061917\n",
      "train loss:0.130457650808\n",
      "train loss:0.156771368043\n",
      "train loss:0.0938350952387\n",
      "train loss:0.114203509785\n",
      "train loss:0.113311476833\n",
      "train loss:0.122849890208\n",
      "train loss:0.140925010023\n",
      "train loss:0.183108798586\n",
      "train loss:0.284812729976\n",
      "train loss:0.159395978167\n",
      "train loss:0.13221257019\n",
      "train loss:0.222112948181\n",
      "train loss:0.127091748294\n",
      "train loss:0.112610146347\n",
      "train loss:0.181451019076\n",
      "train loss:0.120462241974\n",
      "train loss:0.169443122424\n",
      "train loss:0.113870627589\n",
      "train loss:0.168802561627\n",
      "train loss:0.134855300827\n",
      "train loss:0.125309318413\n",
      "train loss:0.104095426161\n",
      "train loss:0.10803538983\n",
      "train loss:0.0977751513291\n",
      "train loss:0.134109350466\n",
      "train loss:0.108996290283\n",
      "train loss:0.106786336834\n",
      "train loss:0.165502618385\n",
      "train loss:0.108651586657\n",
      "train loss:0.187943575381\n",
      "train loss:0.128669697352\n",
      "train loss:0.122241622109\n",
      "train loss:0.107641306941\n",
      "train loss:0.145189131305\n",
      "train loss:0.126623936638\n",
      "train loss:0.0993835416856\n",
      "train loss:0.141283173598\n",
      "train loss:0.0925996470844\n",
      "train loss:0.158319521061\n",
      "train loss:0.119510693604\n",
      "train loss:0.114469561613\n",
      "train loss:0.12191135886\n",
      "train loss:0.123482707233\n",
      "train loss:0.0772830427881\n",
      "train loss:0.147882965682\n",
      "train loss:0.116595276452\n",
      "train loss:0.107941276201\n",
      "=== epoch:9, train acc:0.945, test acc:0.932 ===\n",
      "train loss:0.123332802895\n",
      "train loss:0.158032163264\n",
      "train loss:0.167884716151\n",
      "train loss:0.113390003651\n",
      "train loss:0.078559420606\n",
      "train loss:0.0883649792625\n",
      "train loss:0.134204084712\n",
      "train loss:0.0998696135848\n",
      "train loss:0.175693995257\n",
      "train loss:0.129896068922\n",
      "train loss:0.0456770375869\n",
      "train loss:0.134477585963\n",
      "train loss:0.101756067259\n",
      "train loss:0.188820128169\n",
      "train loss:0.0955092539486\n",
      "train loss:0.155357407614\n",
      "train loss:0.169324319431\n",
      "train loss:0.119512324081\n",
      "train loss:0.0860635525505\n",
      "train loss:0.0657599708223\n",
      "train loss:0.0891964830231\n",
      "train loss:0.18568795239\n",
      "train loss:0.106305867758\n",
      "train loss:0.136139058019\n",
      "train loss:0.0970896416963\n",
      "train loss:0.0647536696367\n",
      "train loss:0.121737508416\n",
      "train loss:0.109902951805\n",
      "train loss:0.0486214866288\n",
      "train loss:0.215982381566\n",
      "train loss:0.130011180903\n",
      "train loss:0.159736958524\n",
      "train loss:0.0879210766588\n",
      "train loss:0.071659520328\n",
      "train loss:0.155232514262\n",
      "train loss:0.225283599848\n",
      "train loss:0.0980187127582\n",
      "train loss:0.119629203329\n",
      "train loss:0.238395426946\n",
      "train loss:0.164878119515\n",
      "train loss:0.0991233268994\n",
      "train loss:0.0298025373255\n",
      "train loss:0.11315915898\n",
      "train loss:0.0611705104253\n",
      "train loss:0.110632725746\n",
      "train loss:0.0795017200327\n",
      "train loss:0.0663778692342\n",
      "train loss:0.125018407977\n",
      "train loss:0.138181231776\n",
      "train loss:0.0568227260606\n",
      "=== epoch:10, train acc:0.961, test acc:0.933 ===\n",
      "train loss:0.0835487586049\n",
      "train loss:0.0839479261962\n",
      "train loss:0.120398664169\n",
      "train loss:0.0924065764862\n",
      "train loss:0.116177943363\n",
      "train loss:0.120400195988\n",
      "train loss:0.100949207465\n",
      "train loss:0.0667542850425\n",
      "train loss:0.0903626120469\n",
      "train loss:0.0522302412257\n",
      "train loss:0.0362794936023\n",
      "train loss:0.07191823451\n",
      "train loss:0.119938790116\n",
      "train loss:0.132562503128\n",
      "train loss:0.1186952088\n",
      "train loss:0.116340503018\n",
      "train loss:0.110997709612\n",
      "train loss:0.032447095113\n",
      "train loss:0.0756162564663\n",
      "train loss:0.0659502369362\n",
      "train loss:0.0857628173784\n",
      "train loss:0.0661043154435\n",
      "train loss:0.0539865631637\n",
      "train loss:0.0853028445586\n",
      "train loss:0.0960364147445\n",
      "train loss:0.0790462175375\n",
      "train loss:0.0500689419234\n",
      "train loss:0.126541946284\n",
      "train loss:0.102649651927\n",
      "train loss:0.0671507211546\n",
      "train loss:0.101967868503\n",
      "train loss:0.132598184535\n",
      "train loss:0.0868184122342\n",
      "train loss:0.106928821472\n",
      "train loss:0.117592029532\n",
      "train loss:0.0895150681967\n",
      "train loss:0.0337752312594\n",
      "train loss:0.0542254185311\n",
      "train loss:0.0906893219349\n",
      "train loss:0.119321768929\n",
      "train loss:0.0481177032839\n",
      "train loss:0.0896943609108\n",
      "train loss:0.150123069702\n",
      "train loss:0.0556360672902\n",
      "train loss:0.0672517070436\n",
      "train loss:0.0624753325216\n",
      "train loss:0.0950771272155\n",
      "train loss:0.0726543303165\n",
      "train loss:0.0630514616481\n",
      "train loss:0.11189594574\n",
      "=== epoch:11, train acc:0.957, test acc:0.932 ===\n",
      "train loss:0.0758956559441\n",
      "train loss:0.164113819183\n",
      "train loss:0.0941964246556\n",
      "train loss:0.12287061482\n",
      "train loss:0.0564374253782\n",
      "train loss:0.0594527385052\n",
      "train loss:0.0651719752683\n",
      "train loss:0.0531522747053\n",
      "train loss:0.0156794747087\n",
      "train loss:0.0575892624698\n",
      "train loss:0.0937184603562\n",
      "train loss:0.0853587727364\n",
      "train loss:0.111408754594\n",
      "train loss:0.0745083637944\n",
      "train loss:0.0463856286085\n",
      "train loss:0.0771579135498\n",
      "train loss:0.052397471152\n",
      "train loss:0.0493978241459\n",
      "train loss:0.112335904313\n",
      "train loss:0.0541952009892\n",
      "train loss:0.0721689258762\n",
      "train loss:0.0543270108434\n",
      "train loss:0.0783559462228\n",
      "train loss:0.11313323435\n",
      "train loss:0.147747837764\n",
      "train loss:0.0784054985526\n",
      "train loss:0.128049209631\n",
      "train loss:0.0587822080719\n",
      "train loss:0.0289263332423\n",
      "train loss:0.0747547616639\n",
      "train loss:0.153483269655\n",
      "train loss:0.0990943276215\n",
      "train loss:0.12220773872\n",
      "train loss:0.0951900192244\n",
      "train loss:0.0757288256023\n",
      "train loss:0.0791788263696\n",
      "train loss:0.0473801098121\n",
      "train loss:0.157894755382\n",
      "train loss:0.123138584871\n",
      "train loss:0.0707397368808\n",
      "train loss:0.0362667991889\n",
      "train loss:0.0440172443667\n",
      "train loss:0.0527358500866\n",
      "train loss:0.0954736251247\n",
      "train loss:0.0207969499763\n",
      "train loss:0.0468507811927\n",
      "train loss:0.145371286795\n",
      "train loss:0.0703303193082\n",
      "train loss:0.0819115195948\n",
      "train loss:0.0926397708866\n",
      "=== epoch:12, train acc:0.973, test acc:0.939 ===\n",
      "train loss:0.133307253342\n",
      "train loss:0.040964453292\n",
      "train loss:0.0242510675802\n",
      "train loss:0.0357328174923\n",
      "train loss:0.0666807260823\n",
      "train loss:0.044061939265\n",
      "train loss:0.126339891815\n",
      "train loss:0.0951482268349\n",
      "train loss:0.0531685429154\n",
      "train loss:0.0222865473602\n",
      "train loss:0.179734368136\n",
      "train loss:0.0646913181536\n",
      "train loss:0.0621423337438\n",
      "train loss:0.0428232752207\n",
      "train loss:0.0588270423764\n",
      "train loss:0.0796350623816\n",
      "train loss:0.0658776433884\n",
      "train loss:0.167066964315\n",
      "train loss:0.0472426060158\n",
      "train loss:0.0773349862461\n",
      "train loss:0.0438225879037\n",
      "train loss:0.0445116943466\n",
      "train loss:0.0693873437893\n",
      "train loss:0.0492264793841\n",
      "train loss:0.0829044920607\n",
      "train loss:0.0787080290522\n",
      "train loss:0.126931032562\n",
      "train loss:0.127498945624\n",
      "train loss:0.0777805724653\n",
      "train loss:0.0591805146517\n",
      "train loss:0.0430653279328\n",
      "train loss:0.053035144748\n",
      "train loss:0.0655754324667\n",
      "train loss:0.105667603576\n",
      "train loss:0.0726245953842\n",
      "train loss:0.0821107334242\n",
      "train loss:0.0651611447391\n",
      "train loss:0.0204064758494\n",
      "train loss:0.0488807621156\n",
      "train loss:0.0950238186488\n",
      "train loss:0.103858874034\n",
      "train loss:0.0997028829883\n",
      "train loss:0.0500252124471\n",
      "train loss:0.208175169922\n",
      "train loss:0.0941553870693\n",
      "train loss:0.0797716756069\n",
      "train loss:0.0538625228993\n",
      "train loss:0.0439247125228\n",
      "train loss:0.0545471797032\n",
      "train loss:0.0570412390821\n",
      "=== epoch:13, train acc:0.969, test acc:0.951 ===\n",
      "train loss:0.0425859111455\n",
      "train loss:0.028874825513\n",
      "train loss:0.0929734834726\n",
      "train loss:0.101438160551\n",
      "train loss:0.0679274001232\n",
      "train loss:0.103493493428\n",
      "train loss:0.0737771469757\n",
      "train loss:0.174128747985\n",
      "train loss:0.0982521998707\n",
      "train loss:0.0663777793671\n",
      "train loss:0.225750735274\n",
      "train loss:0.0787199599296\n",
      "train loss:0.0599956561131\n",
      "train loss:0.056059311804\n",
      "train loss:0.0610612597422\n",
      "train loss:0.0885578238686\n",
      "train loss:0.0940828485783\n",
      "train loss:0.0773858484245\n",
      "train loss:0.0483750862664\n",
      "train loss:0.205327887048\n",
      "train loss:0.0947000307062\n",
      "train loss:0.0264895925996\n",
      "train loss:0.124999651349\n",
      "train loss:0.0452271677673\n",
      "train loss:0.0640082732673\n",
      "train loss:0.13781209943\n",
      "train loss:0.154293275121\n",
      "train loss:0.0573630896038\n",
      "train loss:0.0882421729501\n",
      "train loss:0.0591588310802\n",
      "train loss:0.0580261922697\n",
      "train loss:0.0617429898964\n",
      "train loss:0.0810878968289\n",
      "train loss:0.0643733614547\n",
      "train loss:0.0495169232176\n",
      "train loss:0.0451683222949\n",
      "train loss:0.0633000200559\n",
      "train loss:0.0612866834376\n",
      "train loss:0.0270877578134\n",
      "train loss:0.0629039463236\n",
      "train loss:0.0183112256345\n",
      "train loss:0.0454992636653\n",
      "train loss:0.0657042054982\n",
      "train loss:0.113099698151\n",
      "train loss:0.0829964443555\n",
      "train loss:0.0540601466475\n",
      "train loss:0.0835975721192\n",
      "train loss:0.0639610898008\n",
      "train loss:0.0696196004121\n",
      "train loss:0.0298062788026\n",
      "=== epoch:14, train acc:0.976, test acc:0.94 ===\n",
      "train loss:0.0783540041995\n",
      "train loss:0.115941066295\n",
      "train loss:0.0275713084367\n",
      "train loss:0.0932186006433\n",
      "train loss:0.0401952484319\n",
      "train loss:0.0713633223954\n",
      "train loss:0.054970043997\n",
      "train loss:0.054688188384\n",
      "train loss:0.14551441883\n",
      "train loss:0.0946317551788\n",
      "train loss:0.0521290259609\n",
      "train loss:0.042473419856\n",
      "train loss:0.10942717069\n",
      "train loss:0.161732245694\n",
      "train loss:0.0434948007413\n",
      "train loss:0.0724315317916\n",
      "train loss:0.0549637237943\n",
      "train loss:0.0701725104275\n",
      "train loss:0.103903902996\n",
      "train loss:0.0888681795587\n",
      "train loss:0.0460575079169\n",
      "train loss:0.0310214869981\n",
      "train loss:0.0824787671775\n",
      "train loss:0.0586157827104\n",
      "train loss:0.0341439076656\n",
      "train loss:0.0468888367401\n",
      "train loss:0.0252052412381\n",
      "train loss:0.0721286739028\n",
      "train loss:0.23761375595\n",
      "train loss:0.0504068695831\n",
      "train loss:0.0883990396041\n",
      "train loss:0.0311468844389\n",
      "train loss:0.0423708470874\n",
      "train loss:0.0776174289025\n",
      "train loss:0.0812597001454\n",
      "train loss:0.169961645224\n",
      "train loss:0.0749701761691\n",
      "train loss:0.0242872254931\n",
      "train loss:0.110145226753\n",
      "train loss:0.027168782653\n",
      "train loss:0.104262734899\n",
      "train loss:0.0665720661547\n",
      "train loss:0.0891831565437\n",
      "train loss:0.0560194376855\n",
      "train loss:0.0453357362236\n",
      "train loss:0.0742946935869\n",
      "train loss:0.0482172888439\n",
      "train loss:0.0478082645285\n",
      "train loss:0.0539200250327\n",
      "train loss:0.0531091001735\n",
      "=== epoch:15, train acc:0.982, test acc:0.957 ===\n",
      "train loss:0.0446350511581\n",
      "train loss:0.0502209689196\n",
      "train loss:0.114906126953\n",
      "train loss:0.0660307539644\n",
      "train loss:0.0338253968972\n",
      "train loss:0.0465318082937\n",
      "train loss:0.0773773014802\n",
      "train loss:0.0710271472966\n",
      "train loss:0.106802689568\n",
      "train loss:0.043140679756\n",
      "train loss:0.049059965898\n",
      "train loss:0.0684031615285\n",
      "train loss:0.0479965629425\n",
      "train loss:0.0512501858113\n",
      "train loss:0.0233363688296\n",
      "train loss:0.0564538257853\n",
      "train loss:0.0692737821123\n",
      "train loss:0.0429321195734\n",
      "train loss:0.0509184114774\n",
      "train loss:0.0202349383726\n",
      "train loss:0.0574146214096\n",
      "train loss:0.0579578760706\n",
      "train loss:0.018089708084\n",
      "train loss:0.043050141819\n",
      "train loss:0.03333957413\n",
      "train loss:0.0306832705191\n",
      "train loss:0.023190179139\n",
      "train loss:0.046926408554\n",
      "train loss:0.109707868804\n",
      "train loss:0.0590786330578\n",
      "train loss:0.0301145523505\n",
      "train loss:0.0465500228884\n",
      "train loss:0.0613205766125\n",
      "train loss:0.032496436534\n",
      "train loss:0.0852730851829\n",
      "train loss:0.06679677274\n",
      "train loss:0.055428358908\n",
      "train loss:0.0694892849048\n",
      "train loss:0.0366765489811\n",
      "train loss:0.0357354694822\n",
      "train loss:0.0748338144822\n",
      "train loss:0.0481767583153\n",
      "train loss:0.0310310175308\n",
      "train loss:0.0427349874092\n",
      "train loss:0.0269860610565\n",
      "train loss:0.0168359148127\n",
      "train loss:0.0520773102899\n",
      "train loss:0.0293757307013\n",
      "train loss:0.0414641794451\n",
      "train loss:0.0435335969409\n",
      "=== epoch:16, train acc:0.987, test acc:0.955 ===\n",
      "train loss:0.0594251116224\n",
      "train loss:0.0408316149212\n",
      "train loss:0.0254364623021\n",
      "train loss:0.0273256811378\n",
      "train loss:0.0635050564846\n",
      "train loss:0.0408262078484\n",
      "train loss:0.0239472115723\n",
      "train loss:0.0893213313079\n",
      "train loss:0.0339912443024\n",
      "train loss:0.0169546778157\n",
      "train loss:0.0149036821344\n",
      "train loss:0.0586982580671\n",
      "train loss:0.0620295376092\n",
      "train loss:0.0461492572299\n",
      "train loss:0.0775950569889\n",
      "train loss:0.0146824627669\n",
      "train loss:0.0263530867291\n",
      "train loss:0.0671205310744\n",
      "train loss:0.0520399034271\n",
      "train loss:0.0671486554595\n",
      "train loss:0.0268873851389\n",
      "train loss:0.0168604200145\n",
      "train loss:0.0345112776803\n",
      "train loss:0.0405372190262\n",
      "train loss:0.0731327873506\n",
      "train loss:0.095371707205\n",
      "train loss:0.0354049599024\n",
      "train loss:0.0380499315799\n",
      "train loss:0.0387358844302\n",
      "train loss:0.0210813504237\n",
      "train loss:0.0330928884112\n",
      "train loss:0.0719398066822\n",
      "train loss:0.0348687773642\n",
      "train loss:0.0560607312696\n",
      "train loss:0.029377696622\n",
      "train loss:0.0153692708969\n",
      "train loss:0.0589783539598\n",
      "train loss:0.0244257006507\n",
      "train loss:0.0701388891119\n",
      "train loss:0.0156836549499\n",
      "train loss:0.0713276727434\n",
      "train loss:0.0438458428539\n",
      "train loss:0.0316248906875\n",
      "train loss:0.0423498548445\n",
      "train loss:0.119481001745\n",
      "train loss:0.0282417614236\n",
      "train loss:0.0226165681061\n",
      "train loss:0.120177736091\n",
      "train loss:0.0337980463997\n",
      "train loss:0.0228058099446\n",
      "=== epoch:17, train acc:0.982, test acc:0.952 ===\n",
      "train loss:0.0238031076771\n",
      "train loss:0.0458347164427\n",
      "train loss:0.0658982039016\n",
      "train loss:0.0353855683528\n",
      "train loss:0.0213066005307\n",
      "train loss:0.0206009944427\n",
      "train loss:0.0367718317945\n",
      "train loss:0.0490581624517\n",
      "train loss:0.0167517586155\n",
      "train loss:0.0417375969322\n",
      "train loss:0.0183877063757\n",
      "train loss:0.0576246394113\n",
      "train loss:0.0900485155966\n",
      "train loss:0.0365180955566\n",
      "train loss:0.0353933010316\n",
      "train loss:0.0224199160977\n",
      "train loss:0.0297196417853\n",
      "train loss:0.0540411413188\n",
      "train loss:0.0154329923813\n",
      "train loss:0.0532295302359\n",
      "train loss:0.0315862230617\n",
      "train loss:0.0267650053119\n",
      "train loss:0.0837831943296\n",
      "train loss:0.0277595393841\n",
      "train loss:0.0230340796967\n",
      "train loss:0.0249531746995\n",
      "train loss:0.0292573514215\n",
      "train loss:0.0786016756862\n",
      "train loss:0.0295453299754\n",
      "train loss:0.0163550378579\n",
      "train loss:0.0533281980179\n",
      "train loss:0.0319512508163\n",
      "train loss:0.0208148351752\n",
      "train loss:0.102343456104\n",
      "train loss:0.0152530651325\n",
      "train loss:0.0290760731372\n",
      "train loss:0.0168064386946\n",
      "train loss:0.0181584601416\n",
      "train loss:0.0372073898005\n",
      "train loss:0.015008502099\n",
      "train loss:0.0344468448888\n",
      "train loss:0.0389506776268\n",
      "train loss:0.0540327442114\n",
      "train loss:0.0564625860205\n",
      "train loss:0.0390238006377\n",
      "train loss:0.0156761704374\n",
      "train loss:0.030124077206\n",
      "train loss:0.0330792861376\n",
      "train loss:0.0295121714675\n",
      "train loss:0.0162146874691\n",
      "=== epoch:18, train acc:0.982, test acc:0.954 ===\n",
      "train loss:0.0199100987268\n",
      "train loss:0.0501679555339\n",
      "train loss:0.0458364339734\n",
      "train loss:0.0212095365343\n",
      "train loss:0.0190639523037\n",
      "train loss:0.0287582396096\n",
      "train loss:0.0104405126195\n",
      "train loss:0.00808262937095\n",
      "train loss:0.0367365203891\n",
      "train loss:0.0604723506825\n",
      "train loss:0.0467027257989\n",
      "train loss:0.0233215486061\n",
      "train loss:0.0120494813652\n",
      "train loss:0.00880401551921\n",
      "train loss:0.0303372274331\n",
      "train loss:0.0304287958729\n",
      "train loss:0.0234164664907\n",
      "train loss:0.0207479887311\n",
      "train loss:0.102158119886\n",
      "train loss:0.0252070641133\n",
      "train loss:0.0193422011797\n",
      "train loss:0.0371712332303\n",
      "train loss:0.0133867904893\n",
      "train loss:0.0111680516459\n",
      "train loss:0.0657330231706\n",
      "train loss:0.0307447668039\n",
      "train loss:0.0246028692446\n",
      "train loss:0.0190255112438\n",
      "train loss:0.0195738543477\n",
      "train loss:0.0217890181235\n",
      "train loss:0.0158922330686\n",
      "train loss:0.0508466047317\n",
      "train loss:0.0365046926574\n",
      "train loss:0.0138345615752\n",
      "train loss:0.0100967950158\n",
      "train loss:0.0523145181953\n",
      "train loss:0.0164721281423\n",
      "train loss:0.0357272000444\n",
      "train loss:0.0282413202327\n",
      "train loss:0.00651346231333\n",
      "train loss:0.0236717155885\n",
      "train loss:0.0425651137164\n",
      "train loss:0.023728038277\n",
      "train loss:0.0137234351198\n",
      "train loss:0.0718774250214\n",
      "train loss:0.0366514441609\n",
      "train loss:0.0380348217316\n",
      "train loss:0.0282968117635\n",
      "train loss:0.0244548816132\n",
      "train loss:0.037065427505\n",
      "=== epoch:19, train acc:0.987, test acc:0.958 ===\n",
      "train loss:0.0269529619556\n",
      "train loss:0.0251758088181\n",
      "train loss:0.0270572344742\n",
      "train loss:0.0172745910311\n",
      "train loss:0.0396344172109\n",
      "train loss:0.0286076742297\n",
      "train loss:0.0384808063993\n",
      "train loss:0.0290527888824\n",
      "train loss:0.026819920305\n",
      "train loss:0.0467292678047\n",
      "train loss:0.0195226041133\n",
      "train loss:0.0249728517153\n",
      "train loss:0.0142314414769\n",
      "train loss:0.0389999858006\n",
      "train loss:0.02229913387\n",
      "train loss:0.0187394939298\n",
      "train loss:0.01536054028\n",
      "train loss:0.0266576969121\n",
      "train loss:0.0549706249547\n",
      "train loss:0.0346704449839\n",
      "train loss:0.0116978461211\n",
      "train loss:0.081492974747\n",
      "train loss:0.00661032657225\n",
      "train loss:0.0168693134529\n",
      "train loss:0.0255095698959\n",
      "train loss:0.0469082867663\n",
      "train loss:0.0209446092565\n",
      "train loss:0.0368344871233\n",
      "train loss:0.0312420230259\n",
      "train loss:0.0205485240203\n",
      "train loss:0.0391190578409\n",
      "train loss:0.0377351161827\n",
      "train loss:0.0452158413752\n",
      "train loss:0.0322405269889\n",
      "train loss:0.0225379885655\n",
      "train loss:0.0140630633481\n",
      "train loss:0.0472852240546\n",
      "train loss:0.0223039679293\n",
      "train loss:0.0331549930051\n",
      "train loss:0.0594905779714\n",
      "train loss:0.0503135268355\n",
      "train loss:0.104775178175\n",
      "train loss:0.0186652412177\n",
      "train loss:0.00954881313054\n",
      "train loss:0.0129830804729\n",
      "train loss:0.0174161034228\n",
      "train loss:0.025217273675\n",
      "train loss:0.016071261962\n",
      "train loss:0.0528833083422\n",
      "train loss:0.0249113700062\n",
      "=== epoch:20, train acc:0.98, test acc:0.95 ===\n",
      "train loss:0.0318991247428\n",
      "train loss:0.0473443796509\n",
      "train loss:0.0329844176367\n",
      "train loss:0.0134472134879\n",
      "train loss:0.0151422033846\n",
      "train loss:0.026629504046\n",
      "train loss:0.0331338181366\n",
      "train loss:0.00629596234291\n",
      "train loss:0.010307445022\n",
      "train loss:0.0229684250723\n",
      "train loss:0.0153550077552\n",
      "train loss:0.0206551046924\n",
      "train loss:0.0397963940648\n",
      "train loss:0.0334827134922\n",
      "train loss:0.0203814545144\n",
      "train loss:0.0314139107698\n",
      "train loss:0.0267528219124\n",
      "train loss:0.0211306888024\n",
      "train loss:0.0192660488714\n",
      "train loss:0.0765639919602\n",
      "train loss:0.0544192849921\n",
      "train loss:0.0403826589382\n",
      "train loss:0.0454675795009\n",
      "train loss:0.0312740554178\n",
      "train loss:0.0449508901924\n",
      "train loss:0.0206418497995\n",
      "train loss:0.0199197818965\n",
      "train loss:0.0162922922285\n",
      "train loss:0.0123328321861\n",
      "train loss:0.0223470907504\n",
      "train loss:0.0310474884205\n",
      "train loss:0.0147356931668\n",
      "train loss:0.029070123036\n",
      "train loss:0.039717407659\n",
      "train loss:0.0229311181367\n",
      "train loss:0.0181489806578\n",
      "train loss:0.0343338789473\n",
      "train loss:0.0157448448581\n",
      "train loss:0.0139233220632\n",
      "train loss:0.0269519490612\n",
      "train loss:0.0444697103815\n",
      "train loss:0.0169474950688\n",
      "train loss:0.0227283685056\n",
      "train loss:0.0413293627679\n",
      "train loss:0.0144493461763\n",
      "train loss:0.0122104889316\n",
      "train loss:0.0131339187024\n",
      "train loss:0.0136257468588\n",
      "train loss:0.0194094190172\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.96\n"
     ]
    }
   ],
   "source": [
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICIiQ5QURERkyP8HP0XiQEN/ldQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19580ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
